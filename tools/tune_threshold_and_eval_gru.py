# training/tune_threshold_and_eval_gru.py
"""
é’ˆå¯¹ CNN+GRU çš„é˜ˆå€¼è°ƒä¼˜ä¸ä¸€æ¬¡æ€§æµ‹è¯•è¯„ä¼°è„šæœ¬
- è¯»å– YAML é…ç½®ï¼ˆ--configï¼‰
- æ„å»º CNNGRU å¹¶åŠ è½½ --ckpt
- åœ¨éªŒè¯é›†åšä¸¤é˜¶æ®µï¼ˆç²—/ç»†ï¼‰é˜ˆå€¼æ‰«æï¼Œæ‰¾ best_th_val
- ä½¿ç”¨ best_th_val åœ¨æµ‹è¯•é›†åªè¯„ä¼°ä¸€æ¬¡ï¼ˆé¿å…è¯„ä¼°æ³„éœ²ï¼‰
- å¯¼å‡ºï¼šé˜ˆå€¼æ›²çº¿CSV/å›¾ã€æµ‹è¯•é›†æ··æ·†çŸ©é˜µã€å°å¯¹æ¯”è¡¨ã€summary.jsonã€test_preds.csvï¼ˆä¾› bootstrap ä½¿ç”¨ï¼‰
"""

import os
import time
import json
import argparse
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from sklearn import metrics

from training.dataset_loader import get_dataloaders
from models.cnn_gru import CNNGRU


@torch.no_grad()
def predict_proba(model, loader, device):
    model.eval()
    probs, labels = [], []
    for xb, yb in loader:
        xb = xb.to(device, non_blocking=True)
        logits = model(xb)
        p1 = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
        probs.append(p1)
        labels.append(yb.numpy())
    return np.concatenate(labels), np.concatenate(probs)


def sweep_thresholds(y_true, y_prob, ths):
    """è¿”å› DataFrame å’Œæœ€ä½³ç‚¹ (f1, th, p, r)"""
    rows = []
    best = (-1.0, 0.5, 0.0, 0.0)
    for th in ths:
        y_hat = (y_prob >= th).astype(int)
        p = metrics.precision_score(y_true, y_hat, zero_division=0)
        r = metrics.recall_score(y_true, y_hat, zero_division=0)
        f1 = metrics.f1_score(y_true, y_hat)
        rows.append((float(th), float(p), float(r), float(f1)))
        if f1 > best[0]:
            best = (float(f1), float(th), float(p), float(r))
    df = pd.DataFrame(rows, columns=["threshold", "precision", "recall", "f1"])
    return df, best


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default="config/cnn_gru.yaml", help="path to config.yaml")
    ap.add_argument("--ckpt", required=True, help="path to trained checkpoint (best.pth)")
    ap.add_argument("--outdir", default=None, help="output dir (default: results/tuning_gru/<ts>)")
    ap.add_argument("--suppress_scheduler_warning", action="store_true",
                    help="(ä¿ç•™å…¼å®¹) é™é»˜ä¸è°ƒåº¦å™¨ç›¸å…³çš„è­¦å‘Š")
    args = ap.parse_args()

    # è¾“å‡ºç›®å½•
    ts = time.strftime("%Y%m%d-%H%M%S")
    outdir = args.outdir or os.path.join("results", "tuning_gru", ts)
    os.makedirs(outdir, exist_ok=True)

    # è¯»å–é…ç½® & æ„å»ºæ¨¡å‹
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    model_params = cfg.get("model", {}).get("params", {}) or {}
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"â–¶ Using device: {device}")
    print(f"â–¶ OutDir: {outdir}")
    print(f"â–¶ Model params: {model_params}")

    # DataLoaderï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    train_loader, val_loader, test_loader = get_dataloaders(args.config)

    # CNN+GRU æ¨¡å‹
    model = CNNGRU(num_classes=2, **model_params).to(device)

    # åŠ è½½æƒé‡ï¼ˆå…¼å®¹æœ‰/æ—  weights_onlyï¼‰
    try:
        state_dict = torch.load(args.ckpt, map_location=device, weights_only=True)
    except TypeError:
        state_dict = torch.load(args.ckpt, map_location=device)
    model.load_state_dict(state_dict)
    print(f"âœ… Loaded checkpoint: {args.ckpt}")

    # 1) éªŒè¯é›†æ¦‚ç‡ â†’ ä¸¤é˜¶æ®µé€‰é˜ˆå€¼
    y_true_val, y_prob_val = predict_proba(model, val_loader, device)

    # ç²—æœ
    coarse_ths = np.linspace(0.05, 0.95, 37)
    coarse_df, (c_f1, c_th, c_p, c_r) = sweep_thresholds(y_true_val, y_prob_val, coarse_ths)
    coarse_csv = os.path.join(outdir, "threshold_sweep_metrics_coarse_val.csv")
    coarse_df.to_csv(coarse_csv, index=False)
    print(f"ğŸ’¾ (Val) ç²—æœå·²ä¿å­˜: {coarse_csv}")
    print(f"ğŸ” (Val) Coarse best => th={c_th:.3f} | F1={c_f1:.4f} | P={c_p:.4f} | R={c_r:.4f}")

    # ç»†æœï¼ˆÂ±0.05ï¼Œæ­¥é•¿~0.001ï¼‰
    lo = max(0.0, c_th - 0.05)
    hi = min(1.0, c_th + 0.05)
    if hi - lo < 1e-6:
        lo, hi = max(0.0, c_th - 0.02), min(1.0, c_th + 0.02)
    fine_ths = np.round(np.linspace(lo, hi, int((hi - lo) / 0.001) + 1), 3)
    fine_df, (f_f1, best_th_val, f_p, f_r) = sweep_thresholds(y_true_val, y_prob_val, fine_ths)
    fine_csv = os.path.join(outdir, "threshold_sweep_metrics_fine_val.csv")
    fine_df.to_csv(fine_csv, index=False)
    print(f"ğŸ’¾ (Val) ç»†æœå·²ä¿å­˜: {fine_csv}")

    # åˆå¹¶å¹¶ç”»å›¾
    all_df = pd.concat([coarse_df, fine_df]).drop_duplicates("threshold").sort_values("threshold").reset_index(drop=True)
    all_csv = os.path.join(outdir, "threshold_sweep_metrics_all_val.csv")
    all_df.to_csv(all_csv, index=False)
    print(f"ğŸ’¾ (Val) åˆå¹¶å·²ä¿å­˜: {all_csv}")

    plt.figure(figsize=(5, 4))
    plt.plot(all_df["threshold"], all_df["f1"], label="F1")
    plt.plot(all_df["threshold"], all_df["precision"], label="Precision")
    plt.plot(all_df["threshold"], all_df["recall"], label="Recall")
    plt.axvline(best_th_val, linestyle="--", label=f"best_th_val={best_th_val:.3f}")
    plt.xlabel("Threshold"); plt.ylabel("Score"); plt.title("Metrics vs Threshold (VAL)")
    plt.grid(True, ls="--", alpha=0.4)
    plt.legend()
    val_plot = os.path.join(outdir, "metrics_vs_threshold_VAL.png")
    plt.savefig(val_plot, bbox_inches="tight", dpi=150)
    plt.close()
    print(f"ğŸ–¼ï¸ (Val) æ›²çº¿å·²ä¿å­˜: {val_plot}")
    print(f"âœ… (Val) best_th_val={best_th_val:.3f} | F1={f_f1:.4f} | P={f_p:.4f} | R={f_r:.4f}")

    # 2) æµ‹è¯•é›†ï¼šå›ºå®š best_th_val åªè¯„ä¸€æ¬¡
    y_true_test, y_prob_test = predict_proba(model, test_loader, device)
    # åœ¨ y_true_val, y_prob_val = predict_proba(...) ä¹‹ååŠ ï¼š

    def _summ(name, y_true, y_prob):
        qs = np.quantile(y_prob, [0.0, 0.01, 0.05, 0.5, 0.95, 0.99, 1.0])
        pos = y_prob[y_true == 1];
        neg = y_prob[y_true == 0]
        print(f"[{name}] prob quantiles: {qs}")
        print(f"[{name}] pos mean={pos.mean():.4f}, neg mean={neg.mean():.4f}, "
              f"pos>0.5={(pos > 0.5).mean():.4f}, neg>0.5={(neg > 0.5).mean():.6f}")
        try:
            from sklearn.metrics import roc_auc_score, average_precision_score
            print(
                f"[{name}] ROC-AUC={roc_auc_score(y_true, y_prob):.4f}, PR-AUC={average_precision_score(y_true, y_prob):.4f}")
        except Exception as e:
            print(f"[{name}] AUC error: {e}")

    _summ("VAL", y_true_val, y_prob_val)

    # â€¦â€¦åé¢åœ¨ y_true_test, y_prob_test = predict_proba(...) ä¹‹åå†åŠ ï¼š
    _summ("TEST", y_true_test, y_prob_test)

    # ä¿å­˜æ ·æœ¬çº§é¢„æµ‹ï¼ˆä¾› bootstrap ç”¨ï¼‰
    pd.DataFrame({
        "y_true": y_true_test,
        "y_prob": y_prob_test
    }).to_csv(os.path.join(outdir, "test_probs.csv"), index=False)

    y_pred_test = (y_prob_test >= best_th_val).astype(int)
    p = metrics.precision_score(y_true_test, y_pred_test, zero_division=0)
    r = metrics.recall_score(y_true_test, y_pred_test, zero_division=0)
    f1 = metrics.f1_score(y_true_test, y_pred_test)
    print(f"ğŸ¯ (Test@Val-th) th={best_th_val:.3f} => P={p:.4f} R={r:.4f} F1={f1:.4f}")

    # æ··æ·†çŸ©é˜µ
    cm = metrics.confusion_matrix(y_true_test, y_pred_test)
    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)
    fig, ax = plt.subplots(figsize=(4, 4))
    disp.plot(ax=ax, values_format='d', cmap='Blues', colorbar=False)
    plt.title(f"Confusion Matrix (Test, th={best_th_val:.3f})")
    cm_path = os.path.join(outdir, f"confusion_matrix_test_th{best_th_val:.3f}.png")
    plt.savefig(cm_path, bbox_inches='tight', dpi=150)
    plt.close(fig)
    print(f"ğŸ–¼ï¸ (Test) æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")

    # 3) å°å¯¹æ¯”è¡¨ï¼š0.50 / 0.45 / best_th_valï¼ˆåœ¨æµ‹è¯•é›†ï¼‰
    def eval_at(th):
        y_ = (y_prob_test >= th).astype(int)
        return dict(threshold=float(th),
                    precision=float(metrics.precision_score(y_true_test, y_, zero_division=0)),
                    recall=float(metrics.recall_score(y_true_test, y_, zero_division=0)),
                    f1=float(metrics.f1_score(y_true_test, y_)))
    compare_df = pd.DataFrame([
        eval_at(0.50),
        eval_at(float(cfg.get("inference", {}).get("threshold", 0.45))),
        eval_at(best_th_val)
    ])
    cmp_csv = os.path.join(outdir, "threshold_compare_test.csv")
    compare_df.to_csv(cmp_csv, index=False)
    print(f"ğŸ’¾ é˜ˆå€¼å¯¹æ¯”è¡¨å·²ä¿å­˜: {cmp_csv}")

    # 4) ä¿å­˜æµ‹è¯•é›†æ ·æœ¬çº§é¢„æµ‹ï¼ˆé˜ˆå€¼åçš„æ ‡ç­¾ï¼‰ï¼Œä¾› bootstrap_ci ä½¿ç”¨
    test_preds_df = pd.DataFrame({
        "y_true": y_true_test,
        "y_prob": y_prob_test,
        "y_pred": y_pred_test
    })
    test_preds_df.to_csv(os.path.join(outdir, "test_preds.csv"), index=False)

    # 5) æ±‡æ€»ä¿¡æ¯
    summary = {
        "best_th_val": float(best_th_val),
        "val_best": {"f1": float(f_f1), "precision": float(f_p), "recall": float(f_r)},
        "test_at_best_val": {"f1": float(f1), "precision": float(p), "recall": float(r)},
        "ckpt": args.ckpt,
        "config": args.config,
        "outdir": outdir,
        "timestamp": ts,
        "model": "cnn_gru",
        "model_params": model_params
    }
    with open(os.path.join(outdir, "summary.json"), "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ§¾ æ‘˜è¦å·²ä¿å­˜: {os.path.join(outdir, 'summary.json')}")


if __name__ == "__main__":
    main()
