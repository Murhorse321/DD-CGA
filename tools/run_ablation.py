# tools/run_ablation.py
import os
import sys
import json
import shutil
import argparse
from copy import deepcopy
from datetime import datetime

import yaml


def load_yaml(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def dump_yaml(obj, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        yaml.safe_dump(obj, f, allow_unicode=True, sort_keys=False)


def collect_summary(summary_path: str):
    with open(summary_path, "r", encoding="utf-8") as f:
        return json.load(f)


def run_and_capture_summary(config_path: str) -> str:
    """
    å¯åŠ¨è®­ç»ƒï¼ˆtraining/train_cnn_gru.py --config <config_path>ï¼‰ï¼Œ
    å¹¶ä» stdout ä¸­**åªè§£æå½“å‰è¿™æ¬¡ run** çš„ summary è·¯å¾„ã€‚

    ä¼˜å…ˆåŒ¹é…ï¼š
      1) 'Saved summary to <path>/summary.json'
      2) 'Figures:' è¡Œï¼Œå–åé¢çš„ç›®å½•å¹¶æ‹¼æ¥ '/summary.json'

    ä¸å†ä½¿ç”¨â€œæ‰«æ results/figures æœ€æ–°æ–‡ä»¶â€çš„å…œåº•ï¼Œé¿å…è·¨ç»„ä¸²çº¿ã€‚
    """
    env = os.environ.copy()
    env["PYTHONUTF8"] = "1"

    cmd = [sys.executable, "training/train_cnn_gru.py", "--config", config_path]
    print(">>> Running:", " ".join(cmd))
    p = None
    try:
        p = __import__("subprocess").Popen(
            cmd,
            stdout=__import__("subprocess").PIPE,
            stderr=__import__("subprocess").STDOUT,
            text=True,
            encoding="utf-8",
            env=env,
        )
    except Exception as e:
        raise RuntimeError(f"Failed to start training process: {e}")

    summary_path = None
    figures_hint = None

    # é€è¡Œè¯»å– stdoutï¼Œå°½æ—©æ‹¿åˆ°è·¯å¾„
    while True:
        line = p.stdout.readline()
        if not line and p.poll() is not None:
            break
        if not line:
            continue

        # åŒæ­¥æ‰“å°åˆ°æ§åˆ¶å°
        print(line, end="")

        # 1) ç›´æ¥æŠ“ "Saved summary to <.../summary.json>"
        if "Saved summary to " in line:
            path = line.strip().split("Saved summary to ", 1)[-1].strip()
            path = path.replace("\\", "/")
            summary_path = path  # æœŸæœ›å·²æ˜¯ /summary.json

        # 2) å¤‡é€‰ï¼šæŠ“ "Figures:" æç¤º
        if "Figures:" in line:
            # ä¾‹ï¼š "â–¶ Figures:  results/figures\\20250918-224322"
            after = line.split("Figures:", 1)[-1].strip()
            figures_hint = after.replace("\\", "/")

    ret = p.wait()
    if ret != 0:
        raise RuntimeError(f"Training failed with exit code {ret}")

    # ä¼˜å…ˆç”¨â€œSaved summary to â€¦â€
    if summary_path and os.path.isfile(summary_path):
        return summary_path

    # é€€è€Œæ±‚å…¶æ¬¡ï¼šç”¨ Figures ç›®å½•æ¨æ–­
    if figures_hint:
        candidate = figures_hint.rstrip("/") + "/summary.json"
        if os.path.isfile(candidate):
            return candidate

    # å…¨éƒ½æ²¡æœ‰ï¼Œæ˜ç¡®æŠ¥é”™
    raise FileNotFoundError(
        "æœªåœ¨æœ¬æ¬¡ stdout ä¸­è§£æåˆ° summary è·¯å¾„ã€‚"
        "è¯·ç¡®è®¤ training/train_cnn_gru.py ä¼šæ‰“å° "
        "'Saved summary to .../summary.json' æˆ– 'Figures: <dir>'ã€‚"
    )


def write_compare(outputs, out_dir: str):
    os.makedirs(out_dir, exist_ok=True)
    csv_path = os.path.join(out_dir, "compare.csv")
    md_path = os.path.join(out_dir, "compare.md")
    tex_path = os.path.join(out_dir, "compare.tex")

    headers = [
        "exp_name", "sequence_order", "pooling", "bidirectional", "gru_hidden", "lr",
        "Acc", "F1_macro", "PR_AUC", "ROC_AUC",
        "Cfg_th", "Cfg_F1", "Best_th", "Best_F1",
        "Figures"
    ]
    lines = [",".join(headers)]

    def get(d, *keys, default=""):
        cur = d
        for k in keys:
            if cur is None:
                return default
            cur = cur.get(k, None)
        return cur if cur is not None else default

    # Markdown header
    md = []
    md.append("| " + " | ".join(headers) + " |")
    md.append("|" + "|".join(["---"] * len(headers)) + "|")

    # LaTeX table headerï¼ˆç®€æ´ç‰ˆï¼‰
    tex = []
    tex.append("\\begin{tabular}{l l l c c c c c c c c c c l}")
    tex.append("\\hline")
    tex.append(" & ".join(headers) + " \\\\")
    tex.append("\\hline")

    for out in outputs:
        s = out["summary"]
        cfg = out["config"]

        exp_name = out["name"]
        params = get(cfg, "model", "params", default={})
        seq = params.get("sequence_order", "row")
        pool = params.get("pooling", "mean")
        bi = params.get("bidirectional", True)
        gh = params.get("gru_hidden", 128)
        lr = get(cfg, "training", "learning_rate", default="")
        test = s.get("test", {})
        acc = test.get("acc", "")
        f1m = test.get("f1_macro", "")
        pr  = test.get("pr_auc", "")
        roc = test.get("roc_auc", "")

        cfg_th = s.get("threshold_cfg", "")
        cfg_f1 = get(s, "threshold_cfg_metrics", "f1", default="")
        best_th = s.get("threshold_best", "")
        best_f1 = get(s, "threshold_best_metrics", "f1", default="")

        figdir = get(s, "paths", "figures_dir", default="")

        # å…¼å®¹ç©ºå€¼çš„æ ¼å¼åŒ–
        def f4(x):
            try:
                return f"{float(x):.4f}"
            except Exception:
                return ""
        def f3(x):
            try:
                return f"{float(x):.3f}"
            except Exception:
                return ""

        row = [
            exp_name, str(seq), str(pool), str(bi), str(gh), str(lr),
            f4(acc), f4(f1m), f4(pr), f4(roc),
            f3(cfg_th), f4(cfg_f1), f3(best_th), f4(best_f1), figdir
        ]
        lines.append(",".join(row))
        md.append("| " + " | ".join(row) + " |")
        tex.append(" & ".join(row) + " \\\\")

    with open(csv_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    with open(md_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md))
    tex.append("\\hline")
    tex.append("\\end{tabular}")
    with open(tex_path, "w", encoding="utf-8") as f:
        f.write("\n".join(tex))

    print(f"\nâœ… è¾“å‡ºï¼š\n- {csv_path}\n- {md_path}\n- {tex_path}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--base_config", type=str, default="config/cnn_gru.yaml",
                    help="ä½œä¸ºæ¨¡æ¿çš„ CNN+GRU YAML é…ç½®è·¯å¾„")
    ap.add_argument("--out_root", type=str, default="results/ablation",
                    help="å¯¹æ¯”ç»“æœè¾“å‡ºæ ¹ç›®å½•")
    args = ap.parse_args()

    base = load_yaml(args.base_config)

    # ===== 3 ç»„å®éªŒé…ç½®ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰=====
    # B: last pooling, Bi, hid=128, lr=5e-4, row
    B = deepcopy(base)
    B.setdefault("model", {}).setdefault("params", {})
    B["model"]["params"]["pooling"] = "last"
    B["model"]["params"]["sequence_order"] = "row"
    B["model"]["params"]["bidirectional"] = True
    B["model"]["params"]["gru_hidden"] = 128
    B.setdefault("training", {})
    B["training"]["learning_rate"] = 5e-4

    # C: last pooling, Uni, hid=128, lr=5e-4, row
    C = deepcopy(base)
    C.setdefault("model", {}).setdefault("params", {})
    C["model"]["params"]["pooling"] = "last"
    C["model"]["params"]["sequence_order"] = "row"
    C["model"]["params"]["bidirectional"] = False
    C["model"]["params"]["gru_hidden"] = 128
    C.setdefault("training", {})
    C["training"]["learning_rate"] = 5e-4

    # D: last pooling, Bi, hid=128, lr=5e-4, Z-orderï¼ˆå¯æ”¹ "hilbert"ï¼‰
    D = deepcopy(base)
    D.setdefault("model", {}).setdefault("params", {})
    D["model"]["params"]["pooling"] = "last"
    D["model"]["params"]["sequence_order"] = "z"   # å¯æ”¹ "hilbert"
    D["model"]["params"]["bidirectional"] = True
    D["model"]["params"]["gru_hidden"] = 128
    D.setdefault("training", {})
    D["training"]["learning_rate"] = 5e-4

    variants = [
        ("B_GRU_last_bi_row_lr5e-4", B),
        ("C_GRU_last_uni_row_lr5e-4", C),
        ("D_GRU_last_bi_z_lr5e-4", D),
    ]

    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    work_dir = os.path.join("tmp_cfg", f"ablation_{ts}")
    os.makedirs(work_dir, exist_ok=True)

    results_dir = os.path.join(args.out_root, ts)
    os.makedirs(results_dir, exist_ok=True)

    outputs = []
    for name, cfg in variants:
        # === ä¸ºæ¯ç»„æ³¨å…¥ç‹¬ç«‹çš„ log / ckpt æ ¹ç›®å½•ï¼ˆè®­ç»ƒè„šæœ¬ä¼šåœ¨å…¶åæ‹¼æ¥æ—¶é—´æˆ³ï¼‰===
        cfg.setdefault("training", {})
        cfg["training"]["log_dir"]  = os.path.join("results", "logs", name)
        cfg["training"]["ckpt_dir"] = os.path.join("results", "checkpoints", name)

        # å†™å…¥ä¸´æ—¶ YAML
        tmp_cfg_path = os.path.join(work_dir, f"{name}.yaml")
        dump_yaml(cfg, tmp_cfg_path)

        # è¿è¡Œè®­ç»ƒï¼Œå¹¶è·å–â€œæœ¬ç»„â€çš„ summary.json è·¯å¾„
        summary_path = run_and_capture_summary(tmp_cfg_path)
        summary = collect_summary(summary_path)

        # === å½’æ¡£åˆ° results/ablation/<ts>/<name>/ ===
        arch_dir = os.path.join(results_dir, name)
        os.makedirs(arch_dir, exist_ok=True)

        # 1) summary.json
        shutil.copy2(summary_path, os.path.join(arch_dir, "summary.json"))

        # 2) pointers.jsonï¼ˆæŒ‡å‘åŸå§‹è·¯å¾„ï¼‰
        ckpt = summary.get("paths", {}).get("ckpt", "")
        cfg_used = summary.get("paths", {}).get("config", "")
        fig_dir = summary.get("paths", {}).get("figures_dir", "")

        with open(os.path.join(arch_dir, "pointers.json"), "w", encoding="utf-8") as f:
            json.dump({
                "summary_path": summary_path,
                "ckpt": ckpt,
                "config": cfg_used,
                "figures_dir": fig_dir
            }, f, ensure_ascii=False, indent=2)

        # 3) å¯é€‰ä½†æ¨èï¼šå¤åˆ¶ ckpt ä¸ used_configï¼Œå½¢æˆâ€œè‡ªåŒ…å«å½’æ¡£â€
        if ckpt and os.path.isfile(ckpt):
            shutil.copy2(ckpt, os.path.join(arch_dir, os.path.basename(ckpt)))
        if cfg_used and os.path.isfile(cfg_used):
            shutil.copy2(cfg_used, os.path.join(arch_dir, "used_config.yaml"))

        outputs.append({"name": name, "config": cfg, "summary": summary})

    write_compare(outputs, results_dir)
    print("\nğŸ‰ ä¸‰ç»„å®éªŒå·²å®Œæˆï¼Œæ±‡æ€»è¡¨å·²ç”Ÿæˆã€‚")
    print(f"ğŸ“ æ¯ç»„å·²å½’æ¡£è‡³: {results_dir}/<B|C|D>/")


if __name__ == "__main__":
    main()
