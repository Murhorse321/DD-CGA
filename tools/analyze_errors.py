# tools/analyze_errors.py
# -*- coding: utf-8 -*-
"""
è¯¯åˆ¤åˆ†æè„šæœ¬
åŠŸèƒ½ï¼š
1. è¯»å– train_cnn_gru_att.py ç”Ÿæˆçš„ test_preds.csv (åŒ…å«é¢„æµ‹å€¼)
2. è¯»å– åŸå§‹æµ‹è¯•é›† CSV (åŒ…å«å…·ä½“çš„æ”»å‡»ç±»å‹å­—ç¬¦ä¸²ï¼Œå¦‚ 'DrDoS_DNS')
3. åˆå¹¶ä¸¤è€…ï¼Œç»Ÿè®¡æ¯ç§å…·ä½“æ”»å‡»ç±»å‹çš„ï¼šæ ·æœ¬æ•°ã€è¯¯åˆ¤æ•°ã€å‡†ç¡®ç‡/å¬å›ç‡
4. å¯¼å‡ºè¯¦ç»†çš„é”™è¯¯åˆ†å¸ƒè¡¨
"""

import pandas as pd
import argparse
import os
import sys
import yaml


def main():
    parser = argparse.ArgumentParser(description="Analyze misclassifications by attack type")
    parser.add_argument("--pred_file", type=str, required=True,
                        help="Path to test_preds.csv generated by training script")
    parser.add_argument("--config", type=str, default="config/cnn_gru_att.yaml",
                        help="Path to config file (to find original test dataset path)")
    parser.add_argument("--test_file", type=str, default=None,
                        help="Optional: Manual path to test.csv if not using config")

    args = parser.parse_args()

    # 1. æ£€æŸ¥é¢„æµ‹æ–‡ä»¶
    if not os.path.exists(args.pred_file):
        print(f"âŒ Error: Prediction file not found: {args.pred_file}")
        return

    print(f"ğŸ“– Loading predictions from: {args.pred_file}")
    df_pred = pd.read_csv(args.pred_file)
    # df_pred columns: [y_true, y_prob, y_pred]

    # 2. ç¡®å®šåŸå§‹æµ‹è¯•é›†è·¯å¾„
    test_path = args.test_file
    if test_path is None:
        if os.path.exists(args.config):
            with open(args.config, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
            test_path = cfg.get('data', {}).get('test_path')
        else:
            print(f"âŒ Error: Config file not found: {args.config}")
            return

    if not test_path or not os.path.exists(test_path):
        print(f"âŒ Error: Test dataset path invalid: {test_path}")
        return

    print(f"ğŸ“– Loading original test data from: {test_path}")

    # æˆ‘ä»¬åªéœ€è¦ label (å­—ç¬¦ä¸²æ”»å‡»å) å’Œ label_int (çœŸå€¼)
    # usecols ç¨å¾®ä¼˜åŒ–å†…å­˜
    try:
        df_orig = pd.read_csv(test_path, usecols=['label', 'label_int'])
    except ValueError:
        # å¦‚æœåˆ—åä¸å¯¹ï¼Œå°è¯•è¯»å–æ‰€æœ‰
        df_orig = pd.read_csv(test_path)
        if 'label' not in df_orig.columns:
            print("âŒ Error: 'label' column (string) not found in test csv.")
            return

    # 3. å®Œæ•´æ€§æ£€æŸ¥ä¸åˆå¹¶
    if len(df_pred) != len(df_orig):
        print(f"âš ï¸ Warning: Length mismatch! Preds: {len(df_pred)}, Orig: {len(df_orig)}")
        print("   è¿™å¯èƒ½æ˜¯ç”±äº DataLoader çš„ drop_last è®¾ç½®æˆ–æ–‡ä»¶ç‰ˆæœ¬ä¸ä¸€è‡´å¯¼è‡´çš„ã€‚")
        print("   å°†å°è¯•å–äº¤é›†éƒ¨åˆ†è¿›è¡Œåˆ†æ...")
        min_len = min(len(df_pred), len(df_orig))
        df_pred = df_pred.iloc[:min_len]
        df_orig = df_orig.iloc[:min_len]

    # åˆå¹¶ DataFrame (æŒ‰ç´¢å¼•å¯¹é½)
    # é‡ç½®ç´¢å¼•ä»¥ç¡®ä¿å®‰å…¨
    df_merged = pd.concat([
        df_orig[['label']].reset_index(drop=True),
        df_pred.reset_index(drop=True)
    ], axis=1)

    # 4. æ ¸å¿ƒåˆ†æ
    # é”™è¯¯å®šä¹‰: y_true != y_pred
    df_merged['is_error'] = df_merged['y_true'] != df_merged['y_pred']

    # åŒºåˆ† å‡é˜³æ€§ (FP) å’Œ å‡é˜´æ€§ (FN)
    # FP: True=0, Pred=1 (Benign -> Attack)
    # FN: True=1, Pred=0 (Attack -> Benign)
    df_merged['FP'] = (df_merged['y_true'] == 0) & (df_merged['y_pred'] == 1)
    df_merged['FN'] = (df_merged['y_true'] == 1) & (df_merged['y_pred'] == 0)

    # æŒ‰ 'label' åˆ†ç»„ç»Ÿè®¡
    stats = df_merged.groupby('label').agg(
        Total=('label', 'count'),
        Errors=('is_error', 'sum'),
        FP_Count=('FP', 'sum'),
        FN_Count=('FN', 'sum'),
        # è®¡ç®—è¯¥ç±»åˆ«çš„å¹³å‡é¢„æµ‹æ¦‚ç‡ (æŸ¥çœ‹æ¨¡å‹å¯¹è¯¥ç±»çš„å¹³å‡ç½®ä¿¡åº¦)
        Avg_Prob=('y_prob', 'mean')
    ).reset_index()

    # è®¡ç®— é”™è¯¯ç‡ (Error Rate) å’Œ å¬å›ç‡ (Recall)
    # æ³¨æ„ï¼šå¯¹äº Benignï¼ŒRecall å…¶å®æ˜¯ TNR (True Negative Rate)
    # å¯¹äº Attackï¼ŒRecall = 1 - (FN / Total)
    stats['Error_Rate'] = stats['Errors'] / stats['Total']
    stats['Accuracy'] = 1.0 - stats['Error_Rate']

    # æ’åºï¼šæŒ‰é”™è¯¯ç‡ä»é«˜åˆ°ä½ï¼Œå…³æ³¨â€œæœ€éš¾å•ƒçš„éª¨å¤´â€
    stats = stats.sort_values(by='Error_Rate', ascending=False)

    # 5. è¾“å‡ºæŠ¥å‘Š
    print("\n" + "=" * 60)
    print(f"{'Attack Type':<20} | {'Total':<6} | {'Errors':<6} | {'Acc/Recall':<10} | {'Avg Prob':<8}")
    print("-" * 60)

    for _, row in stats.iterrows():
        name = str(row['label'])
        total = int(row['Total'])
        err = int(row['Errors'])
        acc = row['Accuracy']
        prob = row['Avg_Prob']

        # é«˜äº®æ˜¾ç¤ºé”™è¯¯ç‡é«˜çš„
        marker = "ğŸ”´" if acc < 0.99 else "  "
        if acc < 0.90: marker = "âŒ"

        print(f"{marker} {name:<18} | {total:<6} | {err:<6} | {acc:.2%}{'':<4} | {prob:.4f}")

    print("=" * 60)

    # ä¿å­˜ç»“æœ
    out_dir = os.path.dirname(args.pred_file)
    save_path = os.path.join(out_dir, "error_analysis_by_type.csv")
    stats.to_csv(save_path, index=False)
    print(f"\nâœ… Detailed analysis saved to: {save_path}")

    # 6. é¢å¤–æç¤ºï¼šå…¨å±€ FP/FN
    total_fp = df_merged['FP'].sum()
    total_fn = df_merged['FN'].sum()
    print(f"\nGlobal Summary:")
    print(f" - False Positives (è¯¯æŠ¥): {total_fp} (Benign åˆ¤ä¸º Attack)")
    print(f" - False Negatives (æ¼æŠ¥): {total_fn} (Attack åˆ¤ä¸º Benign)")


if __name__ == "__main__":
    main()