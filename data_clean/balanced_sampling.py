# import os
# import pandas as pd
# import random
#
# # é…ç½®å‚æ•°
# #å¹³è¡¡éšæœºæŠ½æ ·BRS
# chunk_dir = 'processed_chunks'
# target_per_class = 250000
# output_file = 'balanced_dataset.csv'
#
# # åˆå§‹åŒ–ç»Ÿè®¡
# normal_samples = []
# attack_samples = []
# normal_count = 0
# attack_count = 0
#
# # éšæœºæ‰“ä¹±å—é¡ºåº
# chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith('.csv')])
# random.shuffle(chunk_files)
#
# print(f"ğŸ“¦ æ€»å…±æœ‰ {len(chunk_files)} ä¸ªæ•°æ®å—ï¼Œå¼€å§‹æ‰§è¡Œå¹³è¡¡æŠ½æ ·...\n")
#
# for idx, file in enumerate(chunk_files):
#     path = os.path.join(chunk_dir, file)
#     df = pd.read_csv(path)
#
#     if 'label' not in df.columns:
#         print(f"âš ï¸ {file} ä¸­æœªæ‰¾åˆ° label åˆ—ï¼Œè·³è¿‡è¯¥å—")
#         continue
#
#     # åˆ†ç¦»ä¸¤ç±»
#     normal_df = df[df['label'] == 0]
#     attack_df = df[df['label'] == 1]
#
#     # æŒ‰éœ€é‡‡æ ·ï¼ˆè‹¥å‰©ä½™ç›®æ ‡ä¸è¶³å½“å‰å—æ•°ï¼ŒæŒ‰éœ€é‡‡æ ·ï¼‰
#     if normal_count < target_per_class:
#         need_n = min(target_per_class - normal_count, len(normal_df))
#         normal_samples.append(normal_df.sample(n=need_n, random_state=42))
#         normal_count += need_n
#
#     if attack_count < target_per_class:
#         need_a = min(target_per_class - attack_count, len(attack_df))
#         attack_samples.append(attack_df.sample(n=need_a, random_state=42))
#         attack_count += need_a
#
#     print(f"âœ… å¤„ç† {file}ï¼šç´¯è®¡ æ­£å¸¸æµé‡={normal_count}ï¼Œæ”»å‡»æµé‡={attack_count}")
#
#     if normal_count >= target_per_class and attack_count >= target_per_class:
#         print("\nğŸ‰ å·²é‡‡é›†è¶³å¤Ÿæ ·æœ¬ï¼Œç»“æŸéå†ã€‚\n")
#         break
#
# # åˆå¹¶å¹¶ä¿å­˜
# final_df = pd.concat(normal_samples + attack_samples, ignore_index=True)
# final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)  # æ‰“ä¹±
# final_df.to_csv(output_file, index=False)
#
# print(f"âœ… å·²æˆåŠŸä¿å­˜å¹³è¡¡æ•°æ®è‡³ï¼š{output_file}")
# print(f"ğŸ”¢ æœ€ç»ˆæ ·æœ¬æ•°é‡ï¼š{len(final_df)}ï¼ˆæ¯ç±» {target_per_class} æ¡ï¼‰")


import os
import pandas as pd
import random

# é…ç½®å‚æ•°
#å¹³è¡¡éšæœºæŠ½æ ·BRS
chunk_dir = 'D:\Desktop\C_G_A\CNN_GRU_ATTENTION\processed_chunks'
target_per_class = 50000
output_file = 'balanced_dataset_test_cnn.csv'

# åˆå§‹åŒ–ç»Ÿè®¡
normal_samples = []
attack_samples = []
normal_count = 0
attack_count = 0

# éšæœºæ‰“ä¹±å—é¡ºåº
chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith('.csv')])
random.shuffle(chunk_files)

print(f"ğŸ“¦ æ€»å…±æœ‰ {len(chunk_files)} ä¸ªæ•°æ®å—ï¼Œå¼€å§‹æ‰§è¡Œå¹³è¡¡æŠ½æ ·...\n")

for idx, file in enumerate(chunk_files):
    path = os.path.join(chunk_dir, file)
    df = pd.read_csv(path)

    if 'label' not in df.columns:
        print(f"âš ï¸ {file} ä¸­æœªæ‰¾åˆ° label åˆ—ï¼Œè·³è¿‡è¯¥å—")
        continue

    # åˆ†ç¦»ä¸¤ç±»
    normal_df = df[df['label'] == 0]
    attack_df = df[df['label'] == 1]

    # æŒ‰éœ€é‡‡æ ·ï¼ˆè‹¥å‰©ä½™ç›®æ ‡ä¸è¶³å½“å‰å—æ•°ï¼ŒæŒ‰éœ€é‡‡æ ·ï¼‰
    if normal_count < target_per_class:
        need_n = min(target_per_class - normal_count, len(normal_df))
        normal_samples.append(normal_df.sample(n=need_n, random_state=42))
        normal_count += need_n

    if attack_count < target_per_class:
        need_a = min(target_per_class - attack_count, len(attack_df))
        attack_samples.append(attack_df.sample(n=need_a, random_state=42))
        attack_count += need_a

    print(f"âœ… å¤„ç† {file}ï¼šç´¯è®¡ æ­£å¸¸æµé‡={normal_count}ï¼Œæ”»å‡»æµé‡={attack_count}")

    if normal_count >= target_per_class and attack_count >= target_per_class:
        print("\nğŸ‰ å·²é‡‡é›†è¶³å¤Ÿæ ·æœ¬ï¼Œç»“æŸéå†ã€‚\n")
        break

# åˆå¹¶å¹¶ä¿å­˜
final_df = pd.concat(normal_samples + attack_samples, ignore_index=True)
final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)  # æ‰“ä¹±
final_df.to_csv(output_file, index=False)

print(f"âœ… å·²æˆåŠŸä¿å­˜å¹³è¡¡æ•°æ®è‡³ï¼š{output_file}")
print(f"ğŸ”¢ æœ€ç»ˆæ ·æœ¬æ•°é‡ï¼š{len(final_df)}ï¼ˆæ¯ç±» {target_per_class} æ¡ï¼‰")
