# 01_clean_and_select.py
# -*- coding: utf-8 -*-

import os
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# è¯·ç¡®ä¿æ­¤å¤„è·¯å¾„æ­£ç¡®
RAW_DATA_DIR = r"E:\CIC-DDoS\CSVS_chart\CSV-03-11"
OUTPUT_DIR = "data/step1_cleaned_1"

# æ‰¹å¤„ç†å¤§å°ï¼šå¦‚æœå†…å­˜ä¾ç„¶æŠ¥é”™ï¼Œå¯å°†æ­¤æ•°å€¼è°ƒå°ï¼ˆå¦‚ 50000ï¼‰
CHUNK_SIZE = 100000

# éœ€è¦ä¸¥æ ¼å‰”é™¤çš„ç‰¹å¾åˆ—è¡¨
DROP_COLS = [
    'unnamed:_0',
    'flow_id',
    'source_ip',
    'source_port',
    'destination_ip',
    'destination_port',
    'timestamp',
    'simillarhttp'
]


# ===========================================

def process_chunk(df):
    """
    å¯¹å•ä¸ªæ•°æ®å—è¿›è¡Œæ¸…æ´—
    """
    # 1. åˆ—åæ ‡å‡†åŒ–
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

    # 2. å‰”é™¤æŒ‡å®šåˆ—
    existing_drop_cols = [c for c in DROP_COLS if c in df.columns]
    if existing_drop_cols:
        df.drop(columns=existing_drop_cols, inplace=True)

    # 3. å¤„ç† Infinity å’Œ NaN
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)

    # 4. å—å†…å»é‡ (æ³¨ï¼šå…¨å±€å»é‡æå…¶è€—è´¹å†…å­˜ï¼Œåœ¨å¤§æ•°æ®é‡ä¸‹é€šå¸¸ä»…åšå—å†…å»é‡æˆ–åç»­å¤„ç†)
    df.drop_duplicates(inplace=True)

    return df


def clean_csv_file(file_path, output_dir):
    filename = os.path.basename(file_path)
    save_path = os.path.join(output_dir, filename)

    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼Œé˜²æ­¢è¿½åŠ å†™å…¥å¯¼è‡´æ•°æ®é‡å¤
    if os.path.exists(save_path):
        os.remove(save_path)

    total_rows = 0
    first_chunk = True

    try:
        # ä½¿ç”¨ chunksize åˆ†å—è¯»å–
        # engine='c' é€šå¸¸æ›´å¿«ï¼Œä½†å¦‚æœé‡åˆ°è§£æé”™è¯¯å¯å°è¯• engine='python'
        with pd.read_csv(file_path, encoding='utf-8', chunksize=CHUNK_SIZE, low_memory=False) as reader:
            for chunk in reader:
                # å¤„ç†å½“å‰å—
                cleaned_chunk = process_chunk(chunk)

                if cleaned_chunk.empty:
                    continue

                rows = len(cleaned_chunk)
                total_rows += rows

                # å†™å…¥æ¨¡å¼ï¼šç¬¬ä¸€å—ç”¨ 'w' å¹¶ä¿ç•™è¡¨å¤´ï¼Œåç»­å—ç”¨ 'a' å¹¶å»é™¤è¡¨å¤´
                if first_chunk:
                    if not os.path.exists(output_dir):
                        os.makedirs(output_dir)
                    cleaned_chunk.to_csv(save_path, index=False, mode='w', header=True)
                    first_chunk = False
                else:
                    cleaned_chunk.to_csv(save_path, index=False, mode='a', header=False)

        return filename, total_rows

    except Exception as e:
        print(f"\nâŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶ä¾ç„¶å‡ºé”™: {e}")
        # å¦‚æœå‡ºé”™ï¼Œå»ºè®®æ£€æŸ¥è¯¥ CSV æ˜¯å¦æŸå
        return filename, 0


def main():
    if not os.path.exists(RAW_DATA_DIR):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹æ•°æ®ç›®å½•: {RAW_DATA_DIR}")
        return

    csv_files = glob.glob(os.path.join(RAW_DATA_DIR, "*.csv"))
    if not csv_files:
        print(f"è­¦å‘Š: åœ¨ç›®å½• {RAW_DATA_DIR} ä¸‹æœªæ‰¾åˆ° .csv æ–‡ä»¶ã€‚")
        return

    print(f"Found {len(csv_files)} csv files, start cleaning (Chunk Mode)...")
    print("-" * 50)

    total_global_rows = 0
    with tqdm(total=len(csv_files)) as pbar:
        for f in csv_files:
            fname, rows = clean_csv_file(f, OUTPUT_DIR)
            total_global_rows += rows
            pbar.set_description(f"Processing {fname}")
            pbar.update(1)

    print("-" * 50)
    print(f"âœ… æ­¥éª¤ä¸€ (ä¿®æ­£ç‰ˆ) å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    print(f"ğŸ“Š æ€»æœ‰æ•ˆæ ·æœ¬æ•°: {total_global_rows}")
    print("è¯·æ£€æŸ¥è¾“å‡ºç›®å½•çš„æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†ã€‚")
    print("ç¡®è®¤æ— æŠ¥é”™åï¼Œè¯·å›å¤â€œå¯ä»¥ç»§ç»­â€ã€‚")


if __name__ == "__main__":
    main()