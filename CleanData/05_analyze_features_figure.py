# 05_analyze_features_vis.py
# -*- coding: utf-8 -*-

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# ================= ğŸ§ª å®éªŒé…ç½® =================
TRAIN_FILE = "data/step4_split/train.csv"
OUTPUT_DIR = "results/feature_analysis"  # å›¾è¡¨ä¿å­˜è·¯å¾„
REPORT_FILE = os.path.join(OUTPUT_DIR, "feature_report.csv")

# ç»˜å›¾é…ç½®
TOP_N_PLOT = 20  # åœ¨æŸ±çŠ¶å›¾ä¸­å±•ç¤ºå‰å¤šå°‘ä¸ªç‰¹å¾
TARGET_FEAT_NUM = 64  # æˆ‘ä»¬çš„ç›®æ ‡ç‰¹å¾æ•° (ç”¨äºåœ¨å›¾ä¸­ç”»æˆªæ–­çº¿)
DPI = 300  # å›¾ç‰‡åˆ†è¾¨ç‡ (300ä¸ºå­¦æœ¯æ‰“å°æ ‡å‡†)


# ä¸ºäº†è®ºæ–‡é€šç”¨æ€§ï¼Œå›¾è¡¨æ ‡ç­¾å»ºè®®ä½¿ç”¨è‹±æ–‡
# å¦‚æœéœ€è¦ä¸­æ–‡ï¼Œè¯·è§£å¼€ä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Šï¼Œå¹¶ç¡®ä¿ç³»ç»Ÿæœ‰ SimHei å­—ä½“
# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['axes.unicode_minus'] = False

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def plot_importance_bar(df_report, output_dir):
    """ç»˜åˆ¶ Top N ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾"""
    plt.figure(figsize=(10, 8))

    # å–å‰ N ä¸ª
    top_data = df_report.head(TOP_N_PLOT).sort_values(by='Importance', ascending=True)

    # ç»˜åˆ¶æ°´å¹³æŸ±çŠ¶å›¾
    bars = plt.barh(top_data['Feature'], top_data['Importance'], color='#3498db', edgecolor='black', alpha=0.7)

    plt.xlabel('Gini Importance Score', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.title(f'Top {TOP_N_PLOT} Most Important Features (Random Forest)', fontsize=14, fontweight='bold')
    plt.grid(axis='x', linestyle='--', alpha=0.5)

    # ä¿å­˜
    save_path = os.path.join(output_dir, "feature_importance_top20.png")
    plt.tight_layout()
    plt.savefig(save_path, dpi=DPI)
    plt.close()
    print(f"   ğŸ“Š [å›¾è¡¨1] ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾å·²ä¿å­˜: {save_path}")


def plot_cumulative_curve(df_report, output_dir):
    """ç»˜åˆ¶ç´¯ç§¯é‡è¦æ€§æ›²çº¿ï¼Œè¯æ˜å‰64ä¸ªç‰¹å¾è¶³å¤Ÿé‡è¦"""
    plt.figure(figsize=(10, 6))

    # è®¡ç®—ç´¯ç§¯å’Œ
    cumulative_importances = np.cumsum(df_report['Importance'])
    x_values = np.arange(len(cumulative_importances)) + 1

    plt.plot(x_values, cumulative_importances, 'r-', linewidth=2, label='Cumulative Importance')
    plt.fill_between(x_values, cumulative_importances, color='red', alpha=0.1)

    # æ ‡è®°æˆ‘ä»¬çš„æˆªæ–­ç‚¹ (64)
    if len(df_report) >= TARGET_FEAT_NUM:
        cum_score_64 = cumulative_importances[TARGET_FEAT_NUM - 1]
        plt.axvline(x=TARGET_FEAT_NUM, color='blue', linestyle='--', label=f'Cut-off @ {TARGET_FEAT_NUM} Features')
        plt.axhline(y=cum_score_64, color='blue', linestyle='--', alpha=0.5)
        plt.text(TARGET_FEAT_NUM + 2, cum_score_64 - 0.05, f'{cum_score_64:.1%} Explained', color='blue',
                 fontweight='bold')

    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Cumulative Importance', fontsize=12)
    plt.title('Cumulative Feature Importance', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, linestyle='--', alpha=0.5)

    save_path = os.path.join(output_dir, "feature_cumulative_importance.png")
    plt.tight_layout()
    plt.savefig(save_path, dpi=DPI)
    plt.close()
    print(f"   ğŸ“Š [å›¾è¡¨2] ç´¯ç§¯é‡è¦æ€§æ›²çº¿å·²ä¿å­˜: {save_path}")


def plot_correlation_heatmap(df, top_features, output_dir):
    """ä»…ç»˜åˆ¶ Top 15 ç‰¹å¾çš„ç›¸å…³æ€§çƒ­åŠ›å›¾ (å…¨é‡ç”»å¤ªä¹±)"""
    plt.figure(figsize=(12, 10))

    # æå– Top 15 ç‰¹å¾çš„æ•°æ®
    top_15 = top_features[:15]
    corr_matrix = df[top_15].corr()

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # åªç”»ä¸‹ä¸‰è§’
    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm',
                square=True, linewidths=.5, cbar_kws={"shrink": .5})

    plt.title('Correlation Matrix of Top 15 Features', fontsize=14, fontweight='bold')

    save_path = os.path.join(output_dir, "feature_correlation_heatmap.png")
    plt.tight_layout()
    plt.savefig(save_path, dpi=DPI)
    plt.close()
    print(f"   ğŸ“Š [å›¾è¡¨3] Topç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")


def main():
    if not os.path.exists(TRAIN_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {TRAIN_FILE}")
        return

    ensure_dir(OUTPUT_DIR)

    print("ğŸš€ å¼€å§‹ç‰¹å¾åˆ†æä¸å¯è§†åŒ–...")
    df = pd.read_csv(TRAIN_FILE)

    # å‡†å¤‡æ•°æ® (æ’é™¤æ ‡ç­¾)
    exclude_cols = ['label', 'label_int']
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    X = df[feature_cols].select_dtypes(include=[np.number])
    y = df['label_int']

    features = X.columns.tolist()
    print(f"   åˆ†æç‰¹å¾æ•°: {len(features)}")

    # 1. è®­ç»ƒéšæœºæ£®æ—
    print("   æ­£åœ¨è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
    rf = RandomForestClassifier(n_estimators=60, max_depth=12, n_jobs=-1, random_state=42)
    rf.fit(X, y)

    # 2. ç”ŸæˆæŠ¥å‘Š
    importances = rf.feature_importances_
    report = pd.DataFrame({'Feature': features, 'Importance': importances})
    report = report.sort_values(by='Importance', ascending=False).reset_index(drop=True)
    report['Rank'] = report.index + 1

    # ä¿å­˜ CSV
    report.to_csv(REPORT_FILE, index=False)
    print(f"   ğŸ’¾ CSV æŠ¥å‘Šå·²ä¿å­˜: {REPORT_FILE}")

    # 3. ç”Ÿæˆå›¾è¡¨
    print("   æ­£åœ¨ç”Ÿæˆå­¦æœ¯çº§å›¾è¡¨...")

    # å›¾1: æŸ±çŠ¶å›¾
    plot_importance_bar(report, OUTPUT_DIR)

    # å›¾2: ç´¯ç§¯æ›²çº¿
    plot_cumulative_curve(report, OUTPUT_DIR)

    # å›¾3: çƒ­åŠ›å›¾ (éœ€è¦ä¼ å…¥åŸå§‹æ•°æ® X ç”¨äºè®¡ç®—ç›¸å…³æ€§)
    plot_correlation_heatmap(X, report['Feature'].tolist(), OUTPUT_DIR)

    print("-" * 50)
    print("âœ… åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ results/feature_analysis æ–‡ä»¶å¤¹ã€‚")
    print("   è¯·å°†ç”Ÿæˆçš„ 'feature_importance_top20.png' å‘ç»™æˆ‘ï¼Œæˆ–è€…å¤åˆ¶ CSV ä¸­çš„å‰20è¡Œã€‚")
    print("   æˆ‘ä»¬å³å°†å†³å®šæœ€ç»ˆçš„ 64 ä¸ªç‰¹å¾ã€‚")


if __name__ == "__main__":
    main()